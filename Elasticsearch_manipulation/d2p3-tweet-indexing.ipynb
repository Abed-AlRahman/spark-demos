{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting elasticsearch\n",
      "  Downloading https://files.pythonhosted.org/packages/4a/33/d0ed32e077f7dc860153fa866fc52ac312886c9890962ff29379aa753dd1/elasticsearch-7.7.1-py2.py3-none-any.whl (99kB)\n",
      "Collecting urllib3>=1.21.1 (from elasticsearch)\n",
      "  Downloading https://files.pythonhosted.org/packages/e1/e5/df302e8017440f111c11cc41a6b432838672f5a70aa29227bf58149dc72f/urllib3-1.25.9-py2.py3-none-any.whl (126kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\hamedabdelhaq\\anaconda3\\envs\\nlp-deep-learning\\lib\\site-packages (from elasticsearch) (2020.4.5.1)\n",
      "Installing collected packages: urllib3, elasticsearch\n",
      "Successfully installed elasticsearch-7.7.1 urllib3-1.25.9\n"
     ]
    }
   ],
   "source": [
    "#!pip install elasticsearch\n",
    "\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch()\n",
    "index_name = \"tweets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sourrce:\n",
    "https://www.kaggle.com/kazanova/sentiment140/data#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'tweets'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create index with mapping\n",
    "if es.indices.exists(index_name):\n",
    "    es.indices.delete(index=index_name)\n",
    "# index settings\n",
    "settings = {\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"filter\": {\n",
    "        \"trigrams_filter\": {\n",
    "          \"type\": \"ngram\",\n",
    "          \"min_gram\": 3,\n",
    "          \"max_gram\": 4\n",
    "        }\n",
    "      },\n",
    "      \"analyzer\": {\n",
    "        \"text_processing\": {\n",
    "          \"type\": \"custom\",\n",
    "          \"tokenizer\": \"standard\",\n",
    "          \"filter\": [\n",
    "            \"lowercase\",\n",
    "            \"trigrams_filter\"\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    ",\"mappings\": {\n",
    "        \"properties\": {\n",
    "          \"date\": {\n",
    "          \"type\": \"text\",\n",
    "          \"fields\": {\n",
    "            \"keyword\": {\n",
    "              \"type\": \"keyword\",\n",
    "              \"ignore_above\": 256\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        \"flag\": {\n",
    "          \"type\": \"text\",\n",
    "          \"fields\": {\n",
    "            \"keyword\": {\n",
    "              \"type\": \"keyword\",\n",
    "              \"ignore_above\": 256\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        \"id\": {\n",
    "          \"type\": \"text\",\n",
    "          \"fields\": {\n",
    "            \"keyword\": {\n",
    "              \"type\": \"keyword\",\n",
    "              \"ignore_above\": 256\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        \"target\": {\n",
    "          \"type\": \"text\",\n",
    "          \"fields\": {\n",
    "            \"keyword\": {\n",
    "              \"type\": \"keyword\",\n",
    "              \"ignore_above\": 256\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        \"text\": {\n",
    "          \"type\": \"text\",\n",
    "          \"fields\": {\n",
    "            \"keyword\": {\n",
    "              \"type\": \"keyword\",\n",
    "              \"ignore_above\": 256\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        \"user\": {\n",
    "          \"type\": \"text\",\n",
    "          \"fields\": {\n",
    "            \"keyword\": {\n",
    "              \"type\": \"keyword\",\n",
    "              \"ignore_above\": 256\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "        }\n",
    "\n",
    "}\n",
    "\n",
    "    }\n",
    "# create index\n",
    "es.indices.create(index=index_name, ignore=400, body=settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_index': 'tweets', '_type': '_doc', '_id': '2193602064', '_version': 1, 'result': 'created', '_shards': {'total': 2, 'successful': 1, 'failed': 0}, '_seq_no': 0, '_primary_term': 1}\n"
     ]
    }
   ],
   "source": [
    "# inserting records\n",
    "\n",
    "# target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "# ids: The id of the tweet ( 2087)\n",
    "# date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "# flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "# user: the user that tweeted (robotickilldozr)\n",
    "# text: the text of the tweet (Lyx is cool)\n",
    "    \n",
    "\n",
    "tweet = {\n",
    "  \"target\": \"4\",\n",
    "  \"id\": \"2193602064\",\n",
    "  \"date\": \"Tue Jun 16 08:40:49 PDT 2009\",\n",
    "  \"flag\": \"NO_QUERY\",\n",
    "  \"user\": \"tinydiamondz\",\n",
    "  \"text\": \"Happy 38th Birthday to my boo of alll time!!! Tupac Amaru Shakur\"\n",
    "}\n",
    "\n",
    "res = es.index(index=index_name, id=tweet['id'], body=tweet)\n",
    "\n",
    "\n",
    "\n",
    "print(res)\n",
    "\n",
    "# Now check http://localhost:9200/tweets/_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import helpers\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "i = 1\n",
    "actions = []\n",
    "with gzip.open('path_of_gzip_file_containing_twitter_data_16000000','rt') as f:\n",
    "    #print(i, len(actions))\n",
    "    for line in f:\n",
    "        try:\n",
    "            if i%10000!=0:\n",
    "                #print('got line', i)\n",
    "                line = line.replace(\"\\'\", \"\\\"\")\n",
    "                actions.push(line)\n",
    "            else:\n",
    "                print(i)\n",
    "        except:\n",
    "            None\n",
    "        i=i+1\n",
    "        \n",
    "        \n",
    "        \n",
    "# Practice:\n",
    "    #1) download the twiiter file and compress it using gzip command\n",
    "    #2) read the tweets oe by one using the above code, then complete it towards performing the follwoing:\n",
    "        # a) create a json tweet and add it to actions list\n",
    "        # b) bulk insert each 10000 into \"tweets\" index\n",
    "    #3) repeate all the above steps using 'scala'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
